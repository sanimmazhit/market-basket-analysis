{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6LRCy7f4tP9HnCHyQ+7J/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanimmazhit/market-basket-analysis/blob/main/notebooks/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qLSOhkMNvyeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57313ac6-6a40-4f54-a02a-bd30e72d7d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n",
            "License(s): CC0-1.0\n",
            "amazon-books-reviews.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  amazon-books-reviews.zip\n",
            "  inflating: Books_rating.csv        \n",
            "  inflating: books_data.csv          \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"***\"\n",
        "os.environ['KAGGLE_KEY'] = \"***\"\n",
        "\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
        "!unzip -n amazon-books-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "os.makedirs('images', exist_ok=True)\n"
      ],
      "metadata": {
        "id": "m7v1UbmGxgah"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup project folders for GitHub and saving outputs\n",
        "\n",
        "# Create required folders\n",
        "import os\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "# Create .gitkeep files to ensure GitHub tracks the folders\n",
        "with open(\"data/.gitkeep\", \"w\") as f:\n",
        "    pass\n",
        "with open(\"images/.gitkeep\", \"w\") as f:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "JdT8ExYcxh4x"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move CSV files into data/ folder\n",
        "!mkdir -p data\n",
        "!mv Books_rating.csv data/\n",
        "!mv books_data.csv data/"
      ],
      "metadata": {
        "id": "8jMgMzmZkITL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load only review text column\n",
        "df = pd.read_csv(\"data/Books_rating.csv\", usecols=[\"review/text\"])\n",
        "\n",
        "# Drop missing or duplicate reviews\n",
        "df = df.dropna().drop_duplicates(subset=[\"review/text\"])\n",
        "\n",
        "# Filter out very short or very long reviews\n",
        "df[\"review_length\"] = df[\"review/text\"].apply(lambda x: len(str(x).split()))\n",
        "df = df[(df[\"review_length\"] >= 10) & (df[\"review_length\"] <= 100)]\n",
        "\n",
        "# Use only 1% for initial processing (safe)\n",
        "df = df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "pn2aetsKkKAI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Word-based Market Basket Analysis"
      ],
      "metadata": {
        "id": "IcHASXdEx1wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "df[\"cleaned_tokens\"] = df[\"review/text\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "HjO5UtdEkNAu",
        "outputId": "9d985cbf-5905-49c1-b2db-68687c08f13c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten and count all tokens\n",
        "all_words = [word for tokens in df[\"cleaned_tokens\"] for word in tokens]\n",
        "common_words = set([w for w, c in Counter(all_words).most_common(1000)])\n",
        "\n",
        "# Filter to keep only common words\n",
        "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda tokens: [w for w in tokens if w in common_words])\n",
        "\n",
        "# Drop empty baskets\n",
        "df = df[df[\"cleaned_tokens\"].str.len() > 0].reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "y-26MDWdkTvM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_baskets = df[\"cleaned_tokens\"].tolist()\n"
      ],
      "metadata": {
        "id": "LJtbhZ_ikVsD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlxtend\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "te = TransactionEncoder()\n",
        "te_ary = te.fit(word_baskets).transform(word_baskets)\n",
        "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)"
      ],
      "metadata": {
        "id": "qrPHXQxSkXsN",
        "outputId": "78a715f4-0166-48fc-df24-e2fe2b641a69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.11/dist-packages (0.23.4)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (3.10.0)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from mlxtend) (1.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.0->mlxtend) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24.2->mlxtend) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.1->mlxtend) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)\n",
        "frequent_itemsets.sort_values(by=\"support\", ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "jlQBA0y8kajI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
        "rules.sort_values(by=\"confidence\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "e0B2X0pbkbeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"images\", exist_ok=True)"
      ],
      "metadata": {
        "id": "Lz0X0KL7ldWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# ✅ Make sure the 'images' folder exists\n",
        "os.makedirs(\"images\", exist_ok=True)\n",
        "\n",
        "# Filter itemsets with exactly 2 items\n",
        "frequent_itemsets[\"length\"] = frequent_itemsets[\"itemsets\"].apply(lambda x: len(x))\n",
        "top_itemsets = frequent_itemsets[frequent_itemsets[\"length\"] == 2]\n",
        "top_itemsets = top_itemsets.sort_values(by=\"support\", ascending=False).head(10)\n",
        "\n",
        "# Create readable itemset names\n",
        "top_itemsets[\"itemset_str\"] = top_itemsets[\"itemsets\"].apply(lambda x: \", \".join(sorted(x)))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_itemsets[\"itemset_str\"], top_itemsets[\"support\"], color=\"skyblue\")\n",
        "plt.xlabel(\"Support\")\n",
        "plt.title(\"Top 10 Frequent Word Pairs\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "\n",
        "# ✅ Save the plot to the 'images/' folder\n",
        "plt.savefig(\"images/frequent_word_pairs.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Hg1_N2KAlfU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_itemsets.to_csv(\"data/frequent_itemsets.csv\", index=False)\n",
        "rules.to_csv(\"data/association_rules.csv\", index=False)"
      ],
      "metadata": {
        "id": "I1wsREq1mcR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: The set of books reviewed by same user"
      ],
      "metadata": {
        "id": "N8YFNpBLrUTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_rating = pd.read_csv(\"data/Books_rating.csv\")\n",
        "print(df_rating.columns.tolist())\n"
      ],
      "metadata": {
        "id": "HN-LxuRyow7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create baskets of books reviewed by the same user\n",
        "# Load user-book data\n",
        "df_users = pd.read_csv(\"data/Books_rating.csv\", usecols=[\"User_id\", \"Title\"])\n",
        "\n",
        "# Drop missing or duplicate user-book entries\n",
        "df_users = df_users.dropna().drop_duplicates()\n",
        "\n",
        "# Group titles reviewed by each user\n",
        "user_baskets = df_users.groupby(\"User_id\")[\"Title\"].apply(list).tolist()\n",
        "\n",
        "# Remove baskets with less than 2 books (not useful for rules)\n",
        "user_baskets = [basket for basket in user_baskets if len(basket) > 1]\n"
      ],
      "metadata": {
        "id": "5kSqaIqUrOru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "\n",
        "# Encode baskets\n",
        "te_user = TransactionEncoder()\n",
        "te_user_ary = te_user.fit(user_baskets).transform(user_baskets)\n",
        "df_user_encoded = pd.DataFrame(te_user_ary, columns=te_user.columns_)\n",
        "\n",
        "# Find frequent book itemsets\n",
        "frequent_books = apriori(df_user_encoded, min_support=0.005, use_colnames=True)\n",
        "frequent_books = frequent_books.sort_values(by=\"support\", ascending=False)\n",
        "frequent_books.head(10)\n"
      ],
      "metadata": {
        "id": "ylRukln2rsj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rules_books = association_rules(frequent_books, metric=\"confidence\", min_threshold=0.3)\n",
        "rules_books = rules_books.sort_values(by=\"confidence\", ascending=False)\n",
        "rules_books.head(10)\n"
      ],
      "metadata": {
        "id": "Eaxgdmjwrton"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter top 10 frequent book pairs\n",
        "frequent_books[\"length\"] = frequent_books[\"itemsets\"].apply(lambda x: len(x))\n",
        "top_books = frequent_books[frequent_books[\"length\"] == 2].head(10)\n",
        "top_books[\"itemset_str\"] = top_books[\"itemsets\"].apply(lambda x: \", \".join(sorted(x)))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_books[\"itemset_str\"], top_books[\"support\"], color=\"salmon\")\n",
        "plt.xlabel(\"Support\")\n",
        "plt.title(\"Top 10 Frequent Book Pairs Reviewed by Same Users\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save image\n",
        "plt.savefig(\"images/frequent_book_pairs.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "trYh0Btprv2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequent_books.to_csv(\"data/frequent_books.csv\", index=False)\n",
        "rules_books.to_csv(\"data/association_rules_books.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "3HISzFbarx9A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}